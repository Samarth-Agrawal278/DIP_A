{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba793ce9-4481-4868-8592-812c11a4d109",
   "metadata": {
    "id": "ba793ce9-4481-4868-8592-812c11a4d109",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "import time\n",
    "import os\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d729a36a-d212-45bd-9a62-59a61477ed39",
   "metadata": {
    "id": "d729a36a-d212-45bd-9a62-59a61477ed39"
   },
   "outputs": [],
   "source": [
    "class SeBlock(nn.Module):\n",
    "    def __init__(self, in_channels, r = 24):\n",
    "        super().__init__()\n",
    "        C = in_channels\n",
    "        self.globpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc1 = nn.Linear(C, C//r, bias=False)\n",
    "        self.fc2 = nn.Linear(C//r, C, bias=False)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.globpool(x)\n",
    "        f = torch.flatten(f,1)\n",
    "        f = self.silu(self.fc1(f))\n",
    "        f = self.sigmoid(self.fc2(f))\n",
    "        f = f[:,:,None,None]\n",
    "\n",
    "        scale = x * f\n",
    "        return scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ac3f036-e421-47d3-826b-273f1aa46497",
   "metadata": {
    "id": "7ac3f036-e421-47d3-826b-273f1aa46497"
   },
   "outputs": [],
   "source": [
    "class paper_MBConv(nn.Module):\n",
    "    def __init__(self, c_in, c_out, kernel_size, stride = 1, k = 6):\n",
    "        super().__init__()\n",
    "        self.add = ((c_in == c_out) and stride == 1)\n",
    "        padding = kernel_size // 2\n",
    "        c_Bottle = c_in * k\n",
    "        self.net = nn.Sequential(nn.Conv2d(c_in, c_Bottle, kernel_size = 1), nn.ReLU(),\n",
    "                            nn.Conv2d(c_Bottle, c_Bottle, kernel_size = kernel_size, padding = padding, stride = stride, groups = c_Bottle), nn.ReLU(),\n",
    "                            nn.Conv2d(c_Bottle, c_out, kernel_size = 1))\n",
    "    def forward(self, X):\n",
    "        Y = self.net(X)\n",
    "        if self.add:\n",
    "            Y += X\n",
    "        return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e0beea5-8f24-471f-89e9-b497af45c9f6",
   "metadata": {
    "id": "2e0beea5-8f24-471f-89e9-b497af45c9f6"
   },
   "outputs": [],
   "source": [
    "class MBConv(nn.Module):\n",
    "    def __init__(self, c_in, c_out, kernel_size, stride = 1, k = 6):\n",
    "        super().__init__()\n",
    "        self.add = ((c_in == c_out) and stride == 1)\n",
    "        padding = kernel_size // 2\n",
    "        c_Bottle = c_in * k\n",
    "        self.net = nn.Sequential(nn.Conv2d(c_in, c_Bottle, kernel_size = 1), nn.LazyBatchNorm2d(), nn.SiLU(),\n",
    "                            nn.Conv2d(c_Bottle, c_Bottle, kernel_size = kernel_size, padding = padding, stride = stride, groups = c_Bottle), nn.LazyBatchNorm2d(), nn.SiLU(),\n",
    "                            SeBlock(c_Bottle),\n",
    "                            nn.Conv2d(c_Bottle, c_out, kernel_size = 1), nn.LazyBatchNorm2d())\n",
    "    def forward(self, X):\n",
    "        Y = self.net(X)\n",
    "        if self.add:\n",
    "            Y += X\n",
    "        return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "357d29cb-d3ab-4fd9-885f-777f4dc9708f",
   "metadata": {
    "id": "357d29cb-d3ab-4fd9-885f-777f4dc9708f"
   },
   "outputs": [],
   "source": [
    "class RSA(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size = kernel_size, padding = padding)\n",
    "\n",
    "    def forward(self, X):\n",
    "        maxPool, _ = torch.max(X, dim = 1, keepdim = True)\n",
    "        avgPool = torch.mean(X, dim = 1, keepdim = True)\n",
    "        Y = torch.cat((avgPool, maxPool), dim = 1)\n",
    "        Y = self.conv1(Y)\n",
    "        Y = torch.sigmoid(Y)\n",
    "        out = X * Y + X\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3ae64b9-b3d6-42d2-a585-65f92d8bfb31",
   "metadata": {
    "id": "c3ae64b9-b3d6-42d2-a585-65f92d8bfb31"
   },
   "outputs": [],
   "source": [
    "class RCA(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.net1 = nn.Sequential(nn.AdaptiveAvgPool2d((1,1)), nn.Conv2d(channels, channels, kernel_size = 1), nn.Sigmoid())\n",
    "        self.net2 = nn.Sequential(nn.AdaptiveMaxPool2d((1,1)), nn.Conv2d(channels, channels, kernel_size = 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, X):\n",
    "        b1 = self.net1(X)\n",
    "        b2 = self.net2(X)\n",
    "        o1 = X * b1\n",
    "        o2 = X * b2\n",
    "        output = o1 + o2 + X\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d18b9e17-bdc0-405d-becc-066ae7ff2c0e",
   "metadata": {
    "id": "d18b9e17-bdc0-405d-becc-066ae7ff2c0e"
   },
   "outputs": [],
   "source": [
    "class featureExtraction(nn.Module):\n",
    "    def __init__(self, kernel_size, channels):\n",
    "        super().__init__()\n",
    "        self.RSA = RSA(kernel_size)\n",
    "        self.RCA = RCA(channels)\n",
    "        self.Gap = nn.AdaptiveAvgPool2d((1,1))\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.Gap(self.RSA(X)), self.Gap(self.RCA(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3316f896-3ea6-4fa7-8641-b5c2a501603a",
   "metadata": {
    "id": "3316f896-3ea6-4fa7-8641-b5c2a501603a"
   },
   "outputs": [],
   "source": [
    "def split_quadrants(x):\n",
    "    B, C, H, W = x.shape\n",
    "    h2, w2 = H // 2, W // 2\n",
    "\n",
    "    q1 = x[:, :, :h2, :w2]   # top-left\n",
    "    q2 = x[:, :, :h2, w2:]   # top-right\n",
    "    q3 = x[:, :, h2:, :w2]   # bottom-left\n",
    "    q4 = x[:, :, h2:, w2:]   # bottom-right\n",
    "\n",
    "    return (q1, q2, q3, q4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44d0d8a6-a3a1-457c-9808-fd62213e2b19",
   "metadata": {
    "id": "44d0d8a6-a3a1-457c-9808-fd62213e2b19"
   },
   "outputs": [],
   "source": [
    "class Middle(nn.Module):\n",
    "    def __init__(self, kernel_size, channels):\n",
    "        super().__init__()\n",
    "        self.Extractor = nn.ModuleList()\n",
    "        for i in range(4):\n",
    "            self.Extractor.append(featureExtraction(kernel_size, channels))\n",
    "        self.globalExtractor = featureExtraction(kernel_size, channels)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, X):\n",
    "        x_quads = split_quadrants(X)\n",
    "        RSA_out = []\n",
    "        RCA_out = []\n",
    "        for i in range(4):\n",
    "            rsa, rca = self.Extractor[i](x_quads[i])\n",
    "            RSA_out.append(rsa)\n",
    "            RCA_out.append(rca)\n",
    "        rsa, rca = self.globalExtractor(X)\n",
    "        RSA_out.append(rsa)\n",
    "        RCA_out.append(rca)\n",
    "        out1 = torch.cat(RSA_out, dim = 1)\n",
    "        out2 = torch.cat(RCA_out, dim = 1)\n",
    "\n",
    "        return self.flatten(out1 + out2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a43f8048-1df9-40e4-8904-a1e8aa84ef90",
   "metadata": {
    "id": "a43f8048-1df9-40e4-8904-a1e8aa84ef90"
   },
   "outputs": [],
   "source": [
    "class AELGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Conv2d(3, 32, kernel_size = 3, stride = 2, padding = 1), nn.LazyBatchNorm2d(), nn.ReLU(),\n",
    "                                 MBConv(32, 16, kernel_size = 3, stride = 1), MBConv(16, 24, kernel_size = 3, stride = 2),\n",
    "                                 MBConv(24, 40, kernel_size = 5, stride = 2),\n",
    "                                 Middle(kernel_size = 7, channels = 40),\n",
    "                                 nn.LazyLinear(256), nn.ReLU(),\n",
    "                                 nn.LazyLinear(128), nn.ReLU(),\n",
    "                                 nn.LazyLinear(80))\n",
    "    def forward(self, X):\n",
    "        return self.net(X)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "_Ne9KDwPnIoT",
   "metadata": {
    "id": "_Ne9KDwPnIoT"
   },
   "outputs": [],
   "source": [
    "class CLAHETransform:\n",
    "    def __init__(self, clip_limit=2.0, tile_grid_size=(8, 8)):\n",
    "        self.clahe = cv2.createCLAHE(\n",
    "            clipLimit=clip_limit,\n",
    "            tileGridSize=tile_grid_size\n",
    "        )\n",
    "\n",
    "    def __call__(self, img: Image.Image) -> Image.Image:\n",
    "        # PIL (RGB) -> numpy\n",
    "        rgb = np.array(img)\n",
    "\n",
    "        # Handle grayscale images safely\n",
    "        if rgb.ndim == 2:\n",
    "            rgb = np.stack([rgb]*3, axis=-1)\n",
    "\n",
    "        # RGB -> BGR\n",
    "        bgr = cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # BGR -> LAB\n",
    "        lab = cv2.cvtColor(bgr, cv2.COLOR_BGR2LAB)\n",
    "        l, a, b = cv2.split(lab)\n",
    "\n",
    "        # CLAHE on luminance\n",
    "        l = self.clahe.apply(l)\n",
    "\n",
    "        # Merge and back to RGB\n",
    "        lab = cv2.merge((l, a, b))\n",
    "        bgr = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        return Image.fromarray(rgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "Ek4Uazv3nqXn",
   "metadata": {
    "id": "Ek4Uazv3nqXn"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    CLAHETransform(clip_limit=2.0, tile_grid_size=(8, 8)),\n",
    "    transforms.Resize((224, 224)),          # adapt to your model\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4799c485-f2b2-4848-9e0a-494af77eb584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'src = \"./Combined_dataset\"          # original raw images\\ndst = \"./Combined_dataset_clahe\"    # new folder to create with CLAHE applied\\nos.makedirs(dst, exist_ok=True)\\n\\nclahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\\n\\n# iterate classes and files\\nfor cls in sorted(os.listdir(src)):\\n    src_cls = os.path.join(src, cls)\\n    if not os.path.isdir(src_cls):\\n        continue\\n    dst_cls = os.path.join(dst, cls)\\n    os.makedirs(dst_cls, exist_ok=True)\\n    for fname in tqdm(sorted(os.listdir(src_cls)), desc=cls, leave=False):\\n        src_path = os.path.join(src_cls, fname)\\n        dst_path = os.path.join(dst_cls, fname)\\n        try:\\n            im = Image.open(src_path).convert(\"RGB\")\\n            rgb = np.array(im)                         # H,W,3\\n            bgr = cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)\\n            lab = cv2.cvtColor(bgr, cv2.COLOR_BGR2LAB)\\n            l, a, b = cv2.split(lab)\\n            l = clahe.apply(l)\\n            lab = cv2.merge((l, a, b))\\n            bgr = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\\n            rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\\n            out = Image.fromarray(rgb)\\n            out.save(dst_path)\\n        except Exception as e:\\n            print(\"skipped\", src_path, \":\", e)\\n\\n# quick sanity\\nfrom torchvision.datasets import ImageFolder\\nds = ImageFolder(root=dst)\\nprint(\"Preprocessed dataset classes:\", len(ds.classes))\\nprint(\"Example classes:\", ds.classes[:10])\\n# --------------------------------------------------------------------'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------ RUN THIS ONCE BEFORE TRAINING ------------------\n",
    "'''src = \"./Combined_dataset\"          # original raw images\n",
    "dst = \"./Combined_dataset_clahe\"    # new folder to create with CLAHE applied\n",
    "os.makedirs(dst, exist_ok=True)\n",
    "\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "\n",
    "# iterate classes and files\n",
    "for cls in sorted(os.listdir(src)):\n",
    "    src_cls = os.path.join(src, cls)\n",
    "    if not os.path.isdir(src_cls):\n",
    "        continue\n",
    "    dst_cls = os.path.join(dst, cls)\n",
    "    os.makedirs(dst_cls, exist_ok=True)\n",
    "    for fname in tqdm(sorted(os.listdir(src_cls)), desc=cls, leave=False):\n",
    "        src_path = os.path.join(src_cls, fname)\n",
    "        dst_path = os.path.join(dst_cls, fname)\n",
    "        try:\n",
    "            im = Image.open(src_path).convert(\"RGB\")\n",
    "            rgb = np.array(im)                         # H,W,3\n",
    "            bgr = cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)\n",
    "            lab = cv2.cvtColor(bgr, cv2.COLOR_BGR2LAB)\n",
    "            l, a, b = cv2.split(lab)\n",
    "            l = clahe.apply(l)\n",
    "            lab = cv2.merge((l, a, b))\n",
    "            bgr = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "            rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "            out = Image.fromarray(rgb)\n",
    "            out.save(dst_path)\n",
    "        except Exception as e:\n",
    "            print(\"skipped\", src_path, \":\", e)\n",
    "\n",
    "# quick sanity\n",
    "from torchvision.datasets import ImageFolder\n",
    "ds = ImageFolder(root=dst)\n",
    "print(\"Preprocessed dataset classes:\", len(ds.classes))\n",
    "print(\"Example classes:\", ds.classes[:10])\n",
    "# --------------------------------------------------------------------'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "jVgIvCbFwXFS",
   "metadata": {
    "id": "jVgIvCbFwXFS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Dataset size: total=11340, train=9072, val=2268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_9768\\424862283.py:72: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_9768\\424862283.py:90: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_9768\\424862283.py:116: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25  time=130.0s  train_loss=3.8155 train_acc=0.0802  val_loss=3.4605 val_acc=0.1160\n",
      "  Saved best model (val_acc=0.1160) to ./aelgnet_best.pth\n",
      "Epoch 2/25  time=138.2s  train_loss=3.2299 train_acc=0.1726  val_loss=3.0762 val_acc=0.2160\n",
      "  Saved best model (val_acc=0.2160) to ./aelgnet_best.pth\n",
      "Epoch 3/25  time=139.3s  train_loss=2.8334 train_acc=0.2543  val_loss=2.7342 val_acc=0.3007\n",
      "  Saved best model (val_acc=0.3007) to ./aelgnet_best.pth\n",
      "Epoch 4/25  time=149.8s  train_loss=2.5245 train_acc=0.3244  val_loss=2.4412 val_acc=0.3593\n",
      "  Saved best model (val_acc=0.3593) to ./aelgnet_best.pth\n",
      "Epoch 5/25  time=148.8s  train_loss=2.2462 train_acc=0.3847  val_loss=2.2841 val_acc=0.4048\n",
      "  Saved best model (val_acc=0.4048) to ./aelgnet_best.pth\n",
      "Epoch 6/25  time=145.1s  train_loss=2.0521 train_acc=0.4329  val_loss=2.2289 val_acc=0.4136\n",
      "  Saved best model (val_acc=0.4136) to ./aelgnet_best.pth\n",
      "Epoch 7/25  time=144.9s  train_loss=1.8746 train_acc=0.4712  val_loss=2.0479 val_acc=0.4753\n",
      "  Saved best model (val_acc=0.4753) to ./aelgnet_best.pth\n",
      "Epoch 8/25  time=142.9s  train_loss=1.7418 train_acc=0.4977  val_loss=1.9487 val_acc=0.4846\n",
      "  Saved best model (val_acc=0.4846) to ./aelgnet_best.pth\n",
      "Epoch 9/25  time=144.3s  train_loss=1.6327 train_acc=0.5269  val_loss=1.7778 val_acc=0.5322\n",
      "  Saved best model (val_acc=0.5322) to ./aelgnet_best.pth\n",
      "Epoch 10/25  time=136.4s  train_loss=1.5060 train_acc=0.5651  val_loss=1.8075 val_acc=0.5247\n",
      "Epoch 11/25  time=122.0s  train_loss=1.3963 train_acc=0.5944  val_loss=1.6905 val_acc=0.5622\n",
      "  Saved best model (val_acc=0.5622) to ./aelgnet_best.pth\n",
      "Epoch 12/25  time=114.6s  train_loss=1.2958 train_acc=0.6190  val_loss=1.6732 val_acc=0.5732\n",
      "  Saved best model (val_acc=0.5732) to ./aelgnet_best.pth\n",
      "Epoch 13/25  time=116.4s  train_loss=1.1895 train_acc=0.6484  val_loss=1.5424 val_acc=0.5776\n",
      "  Saved best model (val_acc=0.5776) to ./aelgnet_best.pth\n",
      "Epoch 14/25  time=114.1s  train_loss=1.1181 train_acc=0.6653  val_loss=1.4539 val_acc=0.6182\n",
      "  Saved best model (val_acc=0.6182) to ./aelgnet_best.pth\n",
      "Epoch 15/25  time=112.9s  train_loss=1.0505 train_acc=0.6804  val_loss=1.4465 val_acc=0.6076\n",
      "Epoch 16/25  time=112.5s  train_loss=0.9983 train_acc=0.6946  val_loss=1.4100 val_acc=0.6318\n",
      "  Saved best model (val_acc=0.6318) to ./aelgnet_best.pth\n",
      "Epoch 17/25  time=113.7s  train_loss=0.9176 train_acc=0.7220  val_loss=1.4476 val_acc=0.6376\n",
      "  Saved best model (val_acc=0.6376) to ./aelgnet_best.pth\n",
      "Epoch 18/25  time=112.8s  train_loss=0.8661 train_acc=0.7381  val_loss=1.3847 val_acc=0.6596\n",
      "  Saved best model (val_acc=0.6596) to ./aelgnet_best.pth\n",
      "Epoch 19/25  time=112.4s  train_loss=0.8055 train_acc=0.7530  val_loss=1.2311 val_acc=0.6781\n",
      "  Saved best model (val_acc=0.6781) to ./aelgnet_best.pth\n",
      "Epoch 20/25  time=113.2s  train_loss=0.7732 train_acc=0.7669  val_loss=1.2942 val_acc=0.6781\n",
      "Epoch 21/25  time=113.9s  train_loss=0.7157 train_acc=0.7795  val_loss=1.2749 val_acc=0.6852\n",
      "  Saved best model (val_acc=0.6852) to ./aelgnet_best.pth\n",
      "Epoch 22/25  time=113.2s  train_loss=0.7215 train_acc=0.7758  val_loss=1.2409 val_acc=0.6821\n",
      "Epoch 23/25  time=115.6s  train_loss=0.7124 train_acc=0.7823  val_loss=1.2328 val_acc=0.6803\n",
      "Epoch 24/25  time=110.7s  train_loss=0.6667 train_acc=0.7967  val_loss=1.2369 val_acc=0.6852\n",
      "Epoch 25/25  time=109.4s  train_loss=0.6574 train_acc=0.7973  val_loss=1.2289 val_acc=0.6839\n"
     ]
    }
   ],
   "source": [
    "# ----- CONFIG: change these as needed -----\n",
    "dataset_root = \"./Combined_dataset_clahe\"   # <- point to preprocessed folder\n",
    "batch_size   = 16\n",
    "num_workers  = 4        # safe now that transforms are picklable\n",
    "epochs       = 25       # your planned full run\n",
    "lr           = 2e-3\n",
    "weight_decay = 1e-4\n",
    "checkpoint_path = \"./aelgnet_best.pth\"\n",
    "# ------------------------------------------\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Transforms: NO CLAHE here (preprocessed already)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.85, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(\n",
    "        brightness=0.1,\n",
    "        contrast=0.1,\n",
    "        saturation=0.1,\n",
    "        hue=0.02\n",
    "    ),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                         std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# Dataset + deterministic 80/20 split\n",
    "full_dataset = ImageFolder(root=dataset_root, transform=transform_train)\n",
    "n = len(full_dataset)\n",
    "train_n = int(0.8 * n)\n",
    "val_n = n - train_n\n",
    "\n",
    "train_dataset = ImageFolder(root=dataset_root, transform=transform_train)\n",
    "val_dataset   = ImageFolder(root=dataset_root, transform=transform_val)\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "indices = list(range(len(train_dataset)))\n",
    "torch.manual_seed(42)\n",
    "indices = torch.randperm(len(indices)).tolist()\n",
    "train_indices = indices[:train_n]\n",
    "val_indices = indices[train_n:]\n",
    "\n",
    "train_ds = Subset(train_dataset, train_indices)\n",
    "val_ds   = Subset(val_dataset, val_indices)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n",
    "                          num_workers=num_workers, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False,\n",
    "                          num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "print(f\"Dataset size: total={n}, train={train_n}, val={val_n}\")\n",
    "\n",
    "# Model, loss, optimizer (AdamW)\n",
    "\n",
    "model = AELGNet().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# Optional: AMP and scheduler (recommended)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler = GradScaler()\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    t0 = time.time()\n",
    "    # ----- train -----\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total = 0\n",
    "\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_corrects += (preds == labels).sum().item()\n",
    "        total += imgs.size(0)\n",
    "\n",
    "    scheduler.step()\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = running_corrects / total\n",
    "\n",
    "    # ----- validate -----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_corrects = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs = imgs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            with autocast():\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() * imgs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_corrects += (preds == labels).sum().item()\n",
    "            val_total += imgs.size(0)\n",
    "\n",
    "    val_loss = val_loss / val_total\n",
    "    val_acc = val_corrects / val_total\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"Epoch {epoch}/{epochs}  time={elapsed:.1f}s  train_loss={epoch_loss:.4f} train_acc={epoch_acc:.4f}  val_loss={val_loss:.4f} val_acc={val_acc:.4f}\")\n",
    "\n",
    "    # save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc\n",
    "        }, checkpoint_path)\n",
    "        print(f\"  Saved best model (val_acc={val_acc:.4f}) to {checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3fe05c-c73d-4467-b164-d6e0234a3629",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
